# -*- coding: utf-8 -*-
"""Session_14_CW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nB3hJoPl1C9QOGAAXnob0OKTTDsMKdaB
"""

import pandas as pd

df = pd.read_csv('/content/Bank Customer Churn Prediction (1).csv')
display(df.head())

display(df.info())

display(df.tail())

df.describe()

df.drop(['customer_id'], axis=1)

X = df.drop('churn', axis=1)
y = df['churn']



display(X.head())

from sklearn.preprocessing import StandardScaler

# Identify numerical columns (excluding the one-hot encoded columns and customer_id)
numerical_cols = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']

# Apply StandardScaler to numerical columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

display(X.head())

import pickle

# Dump the scaler model
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Identify categorical columns
categorical_cols = ['country', 'gender']

# Create a column transformer for one-hot encoding
preprocessor = ColumnTransformer(
    transformers=[('encoder', OneHotEncoder(), categorical_cols)],
    remainder='passthrough' # Keep other columns as they are
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply one-hot encoding to training and test sets
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)


print("Shape of X_train after one-hot encoding:", X_train.shape)
print("Shape of X_test after one-hot encoding:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Logistic Regression model
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# The confusion matrix is already computed and stored in the 'confusion_matrix' variable
# Let's extract the false positives and false negatives

# Assuming the confusion matrix is in the format:
# [[True Negatives (TN), False Positives (FP)],
#  [False Negatives (FN), True Positives (TP)]]

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print(f"False Positives (FP): {fp}")
print(f"False Negatives (FN): {fn}")



from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Shape of original X_train:", X_train.shape)
print("Shape of resampled X_train:", X_train_resampled.shape)
print("Shape of original y_train:", y_train.shape)
print("Shape of resampled y_train:", y_train_resampled.shape)

# Initialize and train the Logistic Regression model on resampled data
model_resampled = LogisticRegression(random_state=42)
model_resampled.fit(X_train_resampled, y_train_resampled)

# Make predictions on the original test set
y_pred_resampled = model_resampled.predict(X_test)

# Evaluate the model
accuracy_resampled = accuracy_score(y_test, y_pred_resampled)
print(f"Accuracy (Resampled): {accuracy_resampled:.2f}")

print("\nClassification Report (Resampled):")
print(classification_report(y_test, y_pred_resampled))

print("\nConfusion Matrix (Resampled):")
print(confusion_matrix(y_test, y_pred_resampled))

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation on the resampled model using the original data
scores_resampled_cv = cross_val_score(model_resampled, X, y, cv=5)

print("Cross-validation scores (Resampled Model):", scores_resampled_cv)
print("Mean cross-validation accuracy (Resampled Model):", scores_resampled_cv.mean())

# Precision is available in the classification report
print("Classification Report (Resampled):")
print(classification_report(y_test, y_pred_resampled))

import pickle

# Save the trained model
with open('logistic_regression_model_resampled.pkl', 'wb') as f:
    pickle.dump(model_resampled, f)

print("Resampled logistic regression model saved successfully!")

import pickle

# Save the trained model (before resampling)
with open('logistic_regression_model_original.pkl', 'wb') as f:
    pickle.dump(model, f)

print("Original logistic regression model saved successfully!")

"""# Task
Create a Streamlit application that uses the saved logistic regression model ("logistic_regression_model_original.pkl") and scaler ("scaler.pkl") to predict customer churn based on user input.

## Install streamlit

### Subtask:
Install the Streamlit library in the environment.

**Reasoning**:
The subtask is to install the Streamlit library. I will use pip to install it.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

"""## Create a python script for the streamlit app

### Subtask:
Write the code for the Streamlit application in a Python file.

**Reasoning**:
Create a new Python file named `app.py` and add the basic structure for a Streamlit application, including necessary imports and the page title.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# import pandas as pd
# 
# st.set_page_config(page_title="Customer Churn Prediction")
# 
# st.title("Customer Churn Prediction")
# 
# # Placeholder for the rest of the app code
# st.write("Please provide customer details to predict churn.")
#

"""**Reasoning**:
Load the saved logistic regression model and scaler from the pickle files into the Streamlit application.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # Load the trained model and scaler
# try:
#     with open('logistic_regression_model_original.pkl', 'rb') as f:
#         model = pickle.load(f)
# except FileNotFoundError:
#     st.error("Error: Model file 'logistic_regression_model_original.pkl' not found. Make sure it's in the same directory.")
#     st.stop()
# 
# try:
#     with open('scaler.pkl', 'rb') as f:
#         scaler = pickle.load(f)
# except FileNotFoundError:
#     st.error("Error: Scaler file 'scaler.pkl' not found. Make sure it's in the same directory.")
#     st.stop()
#

"""**Reasoning**:
Add input fields for the user to enter customer details in the Streamlit application.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# st.header("Enter Customer Details:")
# 
# # Create input fields for each feature
# credit_score = st.number_input("Credit Score", min_value=350, max_value=850, value=650)
# country = st.selectbox("Country", ['France', 'Germany', 'Spain'])
# gender = st.selectbox("Gender", ['Female', 'Male'])
# age = st.number_input("Age", min_value=18, max_value=92, value=38)
# tenure = st.number_input("Tenure (Years)", min_value=0, max_value=10, value=5)
# balance = st.number_input("Balance", min_value=0.0, value=76000.0)
# products_number = st.number_input("Number of Products", min_value=1, max_value=4, value=1)
# credit_card = st.selectbox("Has Credit Card?", [0, 1], format_func=lambda x: 'Yes' if x == 1 else 'No')
# active_member = st.selectbox("Is Active Member?", [0, 1], format_func=lambda x: 'Yes' if x == 1 else 'No')
# estimated_salary = st.number_input("Estimated Salary", min_value=0.0, value=100000.0)
#

"""**Reasoning**:
Process the user input, prepare it for the model by applying one-hot encoding and scaling, make a prediction, and display the prediction result in the Streamlit application.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # Create a button to make predictions
# if st.button("Predict Churn"):
#     # Create a DataFrame from user input
#     user_data = {
#         'credit_score': [credit_score],
#         'country': [country],
#         'gender': [gender],
#         'age': [age],
#         'tenure': [tenure],
#         'balance': [balance],
#         'products_number': [products_number],
#         'credit_card': [credit_card],
#         'active_member': [active_member],
#         'estimated_salary': [estimated_salary],
#     }
#     user_df = pd.DataFrame(user_data)
# 
#     # Apply one-hot encoding to categorical features (Country and Gender)
#     user_df = pd.get_dummies(user_df, columns=['country', 'gender'], drop_first=False)
# 
#     # Ensure all columns present during training are also present in the user input DataFrame
#     # and reorder columns to match the training data
#     # We need to load a sample of the original training data to get the column order
#     # For simplicity, let's assume the columns are in a specific order and add missing dummy columns
#     # based on the original training data columns. In a real app, you'd save the list of columns.
# 
#     # Assuming the original training columns were:
#     # ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary', 'country_Germany', 'country_Spain', 'gender_Male']
#     # We need to make sure user_df has these columns in this order.
# 
#     # Add missing country columns if not present (e.g., if user selects France, Germany and Spain columns might be missing)
#     if 'country_Germany' not in user_df.columns:
#         user_df['country_Germany'] = False
#     if 'country_Spain' not in user_df.columns:
#         user_df['country_Spain'] = False
# 
#     # Add missing gender column if not present
#     if 'gender_Male' not in user_df.columns:
#         user_df['gender_Male'] = False
# 
# 
#     # Define the expected order of columns based on the training data after preprocessing
#     # This order is derived from the X_train.head() output in the notebook
#     expected_columns = ['credit_score', 'age', 'tenure', 'balance', 'products_number',
#                         'credit_card', 'active_member', 'estimated_salary',
#                         'country_Germany', 'country_Spain', 'gender_Male']
# 
#     # Add 'customer_id' back with a dummy value as it was in the training data for scaling
#     user_df['customer_id'] = 0 # Dummy value, it will be dropped later if not used by model
# 
#     # Reorder columns to match the training data (including customer_id for now)
#     # Need to handle potential missing customer_id in expected columns if it was dropped before scaling
#     # Let's check if 'customer_id' was in the columns used for scaling
#     # Based on the notebook, 'customer_id' was NOT in the numerical_cols for scaling.
#     # It was in X_train, but likely dropped before actual model training if not used.
#     # However, the X_train.head() still shows customer_id. Let's assume the model expects it but doesn't use it.
#     # Let's add it to expected_columns for ordering purposes if it was in X before dropping churn.
#     # Examining the X_train.head() output, 'customer_id' was the first column.
# 
#     # Let's refine expected_columns based on X_train.head() which still had customer_id.
#     # If the model was trained on X_train *including* customer_id, we need to pass it.
#     # If the model was trained on X_train *after dropping* customer_id, we should drop it from user_df before prediction.
#     # Based on cell `njDVsI8of7iO`, `df.drop(['customer_id'], axis=1)` was executed but not assigned back to `df`.
#     # However, in cell `5B8krGs0g4WD`, `X = df.drop('churn', axis=1)` was executed, which MEANS X still contains 'customer_id'.
#     # Therefore, the model was trained with 'customer_id' included in X_train. We must include it in user_df.
#     expected_columns_with_id = ['customer_id'] + expected_columns
# 
#     # Ensure all expected columns are in user_df, adding False for missing one-hot encoded columns if any edge case
#     for col in expected_columns_with_id:
#         if col not in user_df.columns:
#             user_df[col] = False # or 0 depending on the column type
# 
#     user_df = user_df[expected_columns_with_id]
# 
# 
#     # Apply the same scaling as used during training to numerical features
#     # Identify numerical columns used for scaling, excluding customer_id
#     numerical_cols_for_scaling = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']
#     user_df[numerical_cols_for_scaling] = scaler.transform(user_df[numerical_cols_for_scaling])
# 
# 
#     # Make prediction
#     prediction = model.predict(user_df)
#     prediction_proba = model.predict_proba(user_df)[:, 1] # Probability of churn
# 
#     st.subheader("Prediction Result:")
#     if prediction[0] == 1:
#         st.error(f"The customer is likely to churn.")
#     else:
#         st.success(f"The customer is likely to stay.")
# 
#     st.write(f"Probability of Churn: {prediction_proba[0]:.2f}")
#



"""**Reasoning**:
Add input fields for the user to enter customer details in the Streamlit application.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# st.header("Enter Customer Details:")
# 
# # Create input fields for each feature
# credit_score = st.number_input("Credit Score", min_value=350, max_value=850, value=650)
# country = st.selectbox("Country", ['France', 'Germany', 'Spain'])
# gender = st.selectbox("Gender", ['Female', 'Male'])
# age = st.number_input("Age", min_value=18, max_value=92, value=38)
# tenure = st.number_input("Tenure (Years)", min_value=0, max_value=10, value=5)
# balance = st.number_input("Balance", min_value=0.0, value=76000.0)
# products_number = st.number_input("Number of Products", min_value=1, max_value=4, value=1)
# credit_card = st.selectbox("Has Credit Card?", [0, 1], format_func=lambda x: 'Yes' if x == 1 else 'No')
# active_member = st.selectbox("Is Active Member?", [0, 1], format_func=lambda x: 'Yes' if x == 1 else 'No')
# estimated_salary = st.number_input("Estimated Salary", min_value=0.0, value=100000.0)

"""**Reasoning**:
Process the user input, prepare it for the model by applying one-hot encoding and scaling, make a prediction, and display the prediction result in the Streamlit application.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile -a app.py
# 
# # Create a button to make predictions
# if st.button("Predict Churn"):
#     # Create a DataFrame from user input
#     user_data = {
#         'credit_score': [credit_score],
#         'country': [country],
#         'gender': [gender],
#         'age': [age],
#         'tenure': [tenure],
#         'balance': [balance],
#         'products_number': [products_number],
#         'credit_card': [credit_card],
#         'active_member': [active_member],
#         'estimated_salary': [estimated_salary],
#     }
#     user_df = pd.DataFrame(user_data)
# 
#     # Apply one-hot encoding to categorical features (Country and Gender)
#     user_df = pd.get_dummies(user_df, columns=['country', 'gender'], drop_first=False)
# 
#     # Ensure all columns present during training are also present in the user input DataFrame
#     # and reorder columns to match the training data
#     # We need to load a sample of the original training data to get the column order
#     # For simplicity, let's assume the columns are in a specific order and add missing dummy columns
#     # based on the original training data columns. In a real app, you'd save the list of columns.
# 
#     # Assuming the original training columns were:
#     # ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'credit_card', 'active_member', 'estimated_salary', 'country_Germany', 'country_Spain', 'gender_Male']
#     # We need to make sure user_df has these columns in this order.
# 
#     # Add missing country columns if not present (e.g., if user selects France, Germany and Spain columns might be missing)
#     if 'country_Germany' not in user_df.columns:
#         user_df['country_Germany'] = False
#     if 'country_Spain' not in user_df.columns:
#         user_df['country_Spain'] = False
# 
#     # Add missing gender column if not present
#     if 'gender_Male' not in user_df.columns:
#         user_df['gender_Male'] = False
# 
# 
#     # Define the expected order of columns based on the training data after preprocessing
#     # This order is derived from the X_train.head() output in the notebook
#     expected_columns = ['credit_score', 'age', 'tenure', 'balance', 'products_number',
#                         'credit_card', 'active_member', 'estimated_salary',
#                         'country_Germany', 'country_Spain', 'gender_Male']
# 
#     # Add 'customer_id' back with a dummy value as it was in the training data for scaling
#     user_df['customer_id'] = 0 # Dummy value, it will be dropped later if not used by model
# 
#     # Reorder columns to match the training data (including customer_id for now)
#     # Need to handle potential missing customer_id in expected columns if it was dropped before scaling
#     # Let's check if 'customer_id' was in the columns used for scaling
#     # Based on the notebook, 'customer_id' was NOT in the numerical_cols for scaling.
#     # It was in X_train, but likely dropped before actual model training if not used.
#     # However, the X_train.head() still shows customer_id. Let's assume the model expects it but doesn't use it.
#     # Let's add it to expected_columns for ordering purposes if it was in X before dropping churn.
#     # Examining the X_train.head() output, 'customer_id' was the first column.
# 
#     # Let's refine expected_columns based on X_train.head() which still had customer_id.
#     # If the model was trained on X_train *including* customer_id, we need to pass it.
#     # If the model was trained on X_train *after dropping* customer_id, we should drop it from user_df before prediction.
#     # Based on cell `njDVsI8of7iO`, `df.drop(['customer_id'], axis=1)` was executed but not assigned back to `df`.
#     # However, in cell `5B8krGs0g4WD`, `X = df.drop('churn', axis=1)` was executed, which MEANS X still contains 'customer_id'.
#     # Therefore, the model was trained with 'customer_id' included in X_train. We must include it in user_df.
#     expected_columns_with_id = ['customer_id'] + expected_columns
# 
#     # Ensure all expected columns are in user_df, adding False for missing one-hot encoded columns if any edge case
#     for col in expected_columns_with_id:
#         if col not in user_df.columns:
#             user_df[col] = False # or 0 depending on the column type
# 
#     user_df = user_df[expected_columns_with_id]
# 
# 
#     # Apply the same scaling as used during training to numerical features
#     # Identify numerical columns used for scaling, excluding customer_id
#     numerical_cols_for_scaling = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']
#     user_df[numerical_cols_for_scaling] = scaler.transform(user_df[numerical_cols_for_scaling])
# 
# 
#     # Make prediction
#     prediction = model.predict(user_df)
#     prediction_proba = model.predict_proba(user_df)[:, 1] # Probability of churn
# 
#     st.subheader("Prediction Result:")
#     if prediction[0] == 1:
#         st.error(f"The customer is likely to churn.")
#     else:
#         st.success(f"The customer is likely to stay.")
# 
#     st.write(f"Probability of Churn: {prediction_proba[0]:.2f}")

!streamlit run app.py & npx localtunnel --port 8501